{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534daf7f-4b6b-4357-9a38-9117f72ce9b4",
   "metadata": {},
   "source": [
    "# ORCA Inference\n",
    "\n",
    "This Colab demonstrates how to load a pre-trained / finetuned ORCA checkpoint, run inference on some offline images and compare the outputs to the true actions.\n",
    "\n",
    "To get started, please install the orca repository and download the most recent model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f7fd1-5b43-480f-b00f-766248d7f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAX_PLATFORMS'] = 'cpu' # Force on CPU\n",
    "\n",
    "import cv2\n",
    "import jax\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm\n",
    "import rlds\n",
    "import mediapy as media\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79053f4-316f-4d2d-81bd-e6e04cfa81bf",
   "metadata": {},
   "source": [
    "# Load Model Checkpoint\n",
    "First, we will load the pre-trained checkpoint using the `load_pretrained()` function. You can simply feed the path to a checkpoint directory. You can optionally specify the checkpoint step -- if none is given, it will load the latest checkpoint.\n",
    "\n",
    "Please set the appropriate checkpoint directory path below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c04953-869d-48a8-a2df-e601324e97e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from orca.model.orca_model import ORCAModel\n",
    "\n",
    "CHECKPOINT_DIR = \"<path_to_checkpoint_dir\"\n",
    "\n",
    "model = ORCAModel.load_pretrained(CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c298ac8f-da06-41d5-a4a5-145c3080231e",
   "metadata": {},
   "source": [
    "# Load Datasets\n",
    "Next, we will load a trajectory from the bridge dataset for testing the model. We will use the publicly available copy in the Open X-Embodiment dataset bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392bd127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RLDS dataset builder\n",
    "builder = tfds.builder_from_directory(builder_dir='gs://gresearch/robotics/bridge/0.1.0/')\n",
    "ds = builder.as_dataset(split='train[:1]')\n",
    "\n",
    "# sample episode + resize to 256x256 (default third-person cam resolution)\n",
    "episode = next(iter(ds))\n",
    "steps = list(episode['steps'])\n",
    "images = [cv2.resize(np.array(step['observation']['image']), (256, 256)) for step in steps]\n",
    "\n",
    "# extract goal image & language instruction\n",
    "goal_image = images[-1]\n",
    "language_instruction = steps[0]['observation']['natural_language_instruction'].numpy().decode()\n",
    "\n",
    "# visualize episode\n",
    "print(f'Instruction: {language_instruction}')\n",
    "media.show_video(images, fps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ffca5",
   "metadata": {},
   "source": [
    "# Run Inference\n",
    "\n",
    "Next, we will run inference over the images in the episode using the loaded model. \n",
    "Below we demonstrate setups for both, goal-conditioned and language-conditioned training.\n",
    "Note that we need to feed inputs of the correct temporal window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad64434",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2\n",
    "\n",
    "# Jit the sample_actions function for speed\n",
    "policy_fn = jax.jit(model.sample_actions)\n",
    "\n",
    "# create `task` dict\n",
    "task = model.create_tasks(goals={\"image_workspace\": goal_image[None]})   # for goal-conditioned\n",
    "task = model.create_tasks(texts=[language_instruction])                  # for language conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference loop, this model only uses single image observations for bridge\n",
    "# collect predicted and true actions\n",
    "pred_actions, true_actions = [], []\n",
    "for step in range(tqdm.tqdm(len(images) - WINDOW_SIZE + 1)):\n",
    "    input_images = np.stack(images[step : step + WINDOW_SIZE])[None]\n",
    "    observation = {\n",
    "        'image_workspace': input_images,\n",
    "        'pad_mask': np.array([[True, True]]),\n",
    "    }\n",
    "    \n",
    "    # this returns *normalized* actions --> we need to unnormalize using the dataset statistics\n",
    "    norm_actions = policy_fn(observation, task, rng=jax.random.PRNGKey(0))\n",
    "    norm_actions = norm_actions[0]   # remove batch\n",
    "    \n",
    "    actions = (\n",
    "        norm_actions * model.dataset_statistics['action']['std']\n",
    "        + model.dataset_statistics['action']['mean']\n",
    "    )\n",
    "    \n",
    "    pred_actions.append(actions)\n",
    "    true_actions.append(np.concatenate(\n",
    "        (\n",
    "            steps[step+1]['action']['world_vector'], \n",
    "            steps[step+1]['action']['rotation_delta'], \n",
    "            np.array(steps[step+1]['action']['open_gripper']).float()[None]\n",
    "        ), axis=-1\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5e3f7",
   "metadata": {},
   "source": [
    "# Visualize predictions and ground-truth actions\n",
    "\n",
    "Finally, we will visualize the predicted actions in comparison to the groundtruth actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ACTION_DIM_LABELS = ['x', 'y', 'z', 'yaw', 'pitch', 'roll', 'grasp']\n",
    "\n",
    "# build image strip to show above actions\n",
    "img_strip = np.concatenate(np.array(images[::3]), axis=1)\n",
    "\n",
    "# set up plt figure\n",
    "figure_layout = [\n",
    "    ['image'] * len(ACTION_DIM_LABELS),\n",
    "    ACTION_DIM_LABELS\n",
    "]\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "fig, axs = plt.subplot_mosaic(figure_layout)\n",
    "fig.set_size_inches([45, 10])\n",
    "\n",
    "# plot actions\n",
    "pred_actions = np.array(pred_actions).squeeze()\n",
    "true_actions = np.array(true_actions).squeeze()\n",
    "for action_dim, action_label in enumerate(ACTION_DIM_LABELS):\n",
    "  axs[action_label].plot(pred_actions[:, action_dim], label='predicted action')\n",
    "  axs[action_label].plot(true_actions[:, action_dim], label='ground truth')\n",
    "  axs[action_label].set_title(action_label)\n",
    "  axs[action_label].set_xlabel('Time in one episode')\n",
    "\n",
    "axs['image'].imshow(img_strip)\n",
    "axs['image'].set_xlabel('Time in one episode (subsampled)')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
